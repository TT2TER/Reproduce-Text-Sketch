{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import lpips\n",
    "from torchvision import transforms\n",
    "from dataloaders import Kodak\n",
    "from PIL import Image\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eric/miniconda3/envs/control2/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/eric/miniconda3/envs/control2/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /home/eric/miniconda3/envs/control2/lib/python3.8/site-packages/lpips/weights/v0.1/alex.pth\n"
     ]
    }
   ],
   "source": [
    "lpips_loss = lpips.LPIPS(net='alex') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ntc_preprocess(image):\n",
    "    # transform = transforms.Compose(\n",
    "    #         [transforms.Grayscale(), transforms.ToTensor()]\n",
    "    #     )\n",
    "    transform = transforms.Compose(\n",
    "            [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    "        )\n",
    "    image = transform(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = Kodak(root='/home/Shared/image_datasets/Kodak/', batch_size=1)\n",
    "dset = dm.test_dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 768, 512])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset[1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg LPIPS=0.664990060031414, bpp=0.035863028632269964\n"
     ]
    }
   ],
   "source": [
    "name = 'Kodak_recon'\n",
    "exp = 'SD_pi+hed'\n",
    "\n",
    "loss = 0\n",
    "bpp = 0\n",
    "for i, im in enumerate(dset):\n",
    "    recon = Image.open(f'recon_examples/{exp}/{name}/{i}_recon_0.png')\n",
    "    recon = ntc_preprocess(recon)\n",
    "    orig = transforms.functional.to_pil_image(im[0])\n",
    "    orig = ntc_preprocess(orig)\n",
    "    loss += lpips_loss(orig.unsqueeze(0), recon.unsqueeze(0)).item()\n",
    "\n",
    "    with open(f'recon_examples/{exp}/{name}/{i}_caption.yaml', 'r') as stream:\n",
    "        f = yaml.safe_load(stream)\n",
    "    bpp += f['bpp_total']\n",
    "print(f'Avg LPIPS={loss/len(dset)}, bpp={bpp/len(dset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q2_ms-ssim: bpp=0.05042, LPIPS=0.51049\n",
    "# q1_ms-ssim: bpp=0.03066, LPIPS=0.5927511\n",
    "# q1_mse : bpp=0.01841, LPIPS=0.62766"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5878, 0.5878, 0.5878], grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lpips_loss(orig.repeat(3, 1, 1, 1), recon.repeat(3, 1, 1, 1)).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "control2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
